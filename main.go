package main

import (
	"embed"
	"github.com/veritone/engines/pkg/aion"
	"github.com/veritone/engines/pkg/engine"
	"strings"
	"sync"

	llama "go-llama.cpp"
)

//go:embed config.yaml
var f embed.FS // Embed the config.yaml file into the binary

var e = engine.NewEngine(f) // Initialize the engine package globally
var log = e.GetLogger()     // setup logger globally (see https://github.com/rs/zerolog#contextual-logging for more info)

var (
	llamaModel *llama.LLama
	once       sync.Once
)

func getLlamaModel() *llama.LLama {
	once.Do(func() {
		var err error
		// Assuming "/app/models/" + model is the correct path to your model
		llamaModel, err = llama.New("/app/models/"+"llama-2-7b.Q5_K_M.gguf", llama.EnableF16Memory, llama.SetContext(20), llama.EnableEmbeddings, llama.SetGPULayers(0))
		if err != nil {
			log.Fatal().Err(err).Msg("Loading the model failed")
		}
	})
	log.Info().Msg("Model loaded successfully.")
	return llamaModel
}

func init() {
	llamaModel = getLlamaModel()
}

func main() {
	// Define the validation contract for the engine's output.
	// If you don't set this, only the master contract will be used.
	e.SetValidationContracts(aion.Contracts.Text)

	// Define the ready function.
	e.Ready(func() bool {
		return true
	})

	// Define the Process function.
	e.Process(func(data *engine.EngineData) (interface{}, error) {
		// Setup context
		// ctx := context.Background()
		// pd := data.GetProcessData() // Process Data needed by the results webhook.

		// Llama setup
		// model := e.Config.Get("llamaModel").String()
		prompt := e.Config.Get("prompt").String()
		threads := e.Config.Get("threads").Int()
		tokens := e.Config.Get("tokens").Int()
		seed := e.Config.Get("seed").Int()

		// Llama run
		// Initialize llama model
		/*l, err := llama.New("/app/models/"+model, llama.EnableF16Memory, llama.SetContext(128), llama.EnableEmbeddings, llama.SetGPULayers(0))
		  if err != nil {
		  	log.Fatal().Err(err).Msg("Loading the model failed")
		  }
		  log.Info().Msg("Model loaded successfully.")*/

		// Channels for communication
		input := make(chan string)
		results := make(chan ResultType)
		var wg sync.WaitGroup

		// Start the goroutine for llama processing
		wg.Add(1)
		go func() {
			defer wg.Done()
			for text := range input {
				var sentenceBuilder strings.Builder
				var fullTextBuilder strings.Builder

				_, err := llamaModel.Predict(text, llama.Debug, llama.SetTokenCallback(func(token string) bool {
					sentenceBuilder.WriteString(token + " ")
					// Assuming '.' indicates the end of a sentence. Adjust according to your model's behavior.
					if strings.HasSuffix(token, ".") {
						sentence := sentenceBuilder.String()
						log.Debug().Str("Generated Sentence", sentence).Msg("Llama script output")
						// log.Debug().Msg(token) // Log each token generated by the llama script
						fullTextBuilder.WriteString(sentence + " ")
						sentenceBuilder.Reset() // Reset for the next sentence
					}
					return true
				}), llama.SetTokens(int(tokens)), llama.SetThreads(int(threads)), llama.SetTopK(90), llama.SetTopP(0.86), llama.SetStopWords("llama"), llama.SetSeed(int(seed)))

				if err != nil {
					log.Error().Err(err).Msg("Llama predict failed")
					continue
				}

				fullText := fullTextBuilder.String()
				// Log the full text after processing the input
				log.Info().Str("Full Generated Text", fullText).Msg("Full text output")

				results <- ResultType{Sentence: sentenceBuilder.String(), FullText: fullText}
			}
		}()

		// Main loop for input and output
		for {
			input <- prompt // Send input to the goroutine

			// Receive and process the results
			result := <-results
			log.Debug().Str("Sentence", result.Sentence).Msg("Generated sentence")
			log.Info().Str("FullText", result.FullText).Msg("Full generated text")

			output := aion.Aion{
				Object: []aion.AionObject{
					{
						Type: aion.TextType,
						Text: result.FullText,
					},
				},
			}
			return output, nil
		}
	})

	e.Run()
}

// ResultType represents the type of result you expect from llama processing
type ResultType struct {
	Sentence string // The sentence generated by the llama script
	FullText string // The full text returned by the llama script
}
